% \section{Limitations \& Future Work}
\section{Status}
% Build-times (mention experimental new build tools (slim, nydus, etc.))
% Dependency Hell (Build/MLflow,etc)
% Occasional Flakiness due to implementation (new to pydantic)

% https://aneescraftsmanship.com/circle-symbols%E2%97%8B%E2%97%8F%E2%97%8D%E2%97%97%E2%97%94%E2%97%99%E2%A6%BF-in-latex/
\begin{itemize}
    \item [\faCircleO] \textbf{Not Implemented} : FLOps does not meet the requirement in its current form. 
	\item [\faDotCircleO] \textbf{Partially Implemented} : FLOps only partially meets the requirement.
	\item [\faArrowCircleRight] \textbf{Implemented MVP} : FLOps fulfils the requirement in a minimal viable way. Features can be further extended.
	\item [\faCircle] \textbf{Fully Implemented} : FLOps fully realized the requirement.
\end{itemize}

\input{tables/conclusion_functional_reqs_status.tex}

Table \ref{table:status_functional_reqs} depicts how well the current FLOps implementation fulfills its functional requirements.
We count FR-1.3 and FR-2 as MVP implemented because FLOps does support different FL scenarios such as classic and clustered HFL as well as various project configurations.
This support is not exhaustive.
There exist multiple additional FL strategies, algorithms, and Flower configurations that can and should be analyzed and added to future FLOps versions to allow a wider range of FL capabilities.
FLOps currently supports Scikit-learn, Pytorch, and Tensorflow.
Other ML flavors can be easily added by extending the underlying minimal enum structure.
We regard FR-6's inference serving as MVP implemented because enabling inference serving is not a primary focus point of FLOps.
Instead inference serving is FLOps currently last post-training step thus we did not invest too much time into this feature.
To be seen as fully implemented it should be thoroughly tested and investigated.
MLflow offers different ways of turing trained models into inference servers.
Analyzing and comparing these variations and letting users decide how to create inference servers is a valid piece of future work.

\input{tables/conclusion_nonfunctional_reqs_status.tex}

% Security & Privacy 
% GPU
% Personalized FL <- mention papers here (make an introduction here)
% Extended/Modified HFL
% More FL - diff directions - diff algos
% More ML Frameworks & Datasets Experiments
% Experimentation with edge focused ML frameworks, PI, etc (multi-platform/edge)
\subsection{Federated Learning via FLOps}

% More automation via more CLI integration, etc
% Integrate with Mahmouds work
% Evaluated primarily on Monolith machine - the resources could be analyzed in greater detail (per container/service) remove host,etc traffic/consumption
\subsection{Complementary Components \& Integrations}