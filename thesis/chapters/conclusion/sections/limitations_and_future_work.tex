% \section{Limitations \& Future Work}
\section{Current Status, Limitations \& Future Work}
% Build-times (mention experimental new build tools (slim, nydus, etc.))
% Dependency Hell (Build/MLflow,etc)
% Occasional Flakiness due to implementation (new to pydantic)

% https://aneescraftsmanship.com/circle-symbols%E2%97%8B%E2%97%8F%E2%97%8D%E2%97%97%E2%97%94%E2%97%99%E2%A6%BF-in-latex/
\begin{itemize}
    \item [\faCircleO] \textbf{Not Implemented}: FLOps does not meet the requirement in its current form. 
	\item [\faDotCircleO] \textbf{Partially Implemented}: FLOps only partially meets the requirement.
	\item [\faArrowCircleRight] \textbf{Implemented MVP}: FLOps fulfils the requirement in a minimal viable way. Features can be further extended.
	\item [\faCircle] \textbf{Fully Implemented}: FLOps fully realizes the requirement.
\end{itemize}

\input{tables/conclusion_functional_reqs_status.tex}

Table \ref{table:status_functional_reqs} depicts how well the current FLOps implementation fulfills its functional requirements.
We count FR-1.3 and FR-2 as MVP implemented because FLOps does support different FL scenarios such as classic and clustered HFL as well as various project configurations.
This support is not exhaustive.
There exist multiple additional FL strategies, algorithms, and Flower configurations that can and should be analyzed and added to future FLOps versions to allow a wider range of FL capabilities.
FLOps currently supports Scikit-learn, Pytorch, and Tensorflow.
Other ML flavors can be easily added by extending the underlying minimal enum structure.
We regard FR-6's inference serving as MVP implemented because enabling inference serving is not a primary focus point of FLOps.
Instead inference serving is FLOps currently last post-training step thus we did not invest too much time into this feature.
To be seen as fully implemented it should be thoroughly tested and investigated.
MLflow offers different ways of turing trained models into inference servers.
Analyzing and comparing these variations and letting users decide how to create inference servers is a valid piece of future work.

\input{tables/conclusion_nonfunctional_reqs_status.tex}

Table \ref{table:status_nonfunctional_reqs} shows the fulfillment grade of nonfunctional requirements by the current FLOps version.
FLOps only partially realizes NFR-2.2.2.
During the development of FLOps we had to introduce several minor changes and additions to Oakestra for it to handle FLOps workflows.
FLOps project structure follows the application and service structure of Oakestra which seems to be different to other orchestators like Kubernetes.
We did not try to run FLOps via another orchestrator like Kubernetes due to the restricted time.
Therefore, we cannot tell how straightforward it is to use FLOps with other orchestrators.
Nontheless, FLOps communicates via strict decoupled APIs and SLAs with Oakestra, thus replacing or extending these interaction points to other orchestrators should be straightforward.
In addition, Jabok Kempter's work (TODO) to combine Oakestra and Kubernets alludes that FLOps can be run on Kubernetes even as a Oakestra addon.
We marked NFR-3.1 as MVP implemented and NFR-3.2 as partially implemented because FLOps is capable of running on monolith and small multi-cluster setups with varing numbers of FL-actors.
We did not try to run FLOps on a truly large scale, such as several hundred or thousand devices yet, thus cannot indefinitely confirm that FLOps is capable of handling large scale deployments.
Regarding availability, FLOps includes several mechanism of communication and error/event handling.
However, if FLOps services or applications are tinkered with or removed/deleted from an external non FLOps source current FLOps is incapable of reacting accordingly.
We hope to change this once FLOps is connected to Mahmoud ElKodary's Oakestra addon marketplace and hooks so it is able to responde and receive these vital events and react accordingly.

\subsubsection{Image Building}
FLOps uses a sophisticated image building architecture and processes yet the evaluations have shown that this aspect is taking up a significant portion of project run times.
To further improve the build times and shrink the final images more newer experimental solutions should be explored.
One way would be to replace the current Conda-Mamba solution with uv \cite{uv}.
Other tools worth exploring to slim down images are found here \cite{slim,dragonfly,nydus}.

\subsubsection{Security \& Privacy}
Security and privacy are foundational concerns of FL.
Our priority with FLOps was to create and verify its foundational architecture and components.
Due to the lack of access to proper certificates and to accelerate FLOps' development we omitted privacy and security concerns.
This includes the use of HTTP instead of HTTPS or the lack of supported FL security features such as secure aggregation.
This aspect is one of the most important for future work.
The good news are that Flower, the image registry, and local data management Apache suite all come with security features that simply need to be configured properly.

\subsubsection{Federated Learning via FLOps}
Classic and clustered HFL is already working via FLOps.
There are many other possible improvements that should be considered for future work.
Possible directions include exploring and adding FL native security and privacy mechanisms or extending the already available functionalities by adding more parameters and different algorithms.
FLOps so far did not focus on GPU workloads, which are straightforward to add via Flower.
In addition, FLOps should be tried out with ML frameworks that are specifically targeted for resource restricted edge and IoT devices.
These experiments should be evaluated on real edge devices such as Raspberry Pis and hybrid setups.

One major aspect that should be investigated is personalized FL.
Flower already supports PFL thus it should be straightforward to enable these features to FLOps.
Custom solutions only using FLOps are also possible.
For classic PFL after training the global model this model could be deployed on every learner and trained further for local use only and then used for inference serving.
The sidecar or multi-model PFL solution could look like this.
The global model would be trained as is but a personal model would also be trained concurrently and not shared or updated with other learners.
Once the training is done the local model could be turned into an inference server.
Hybrids between both are also possible.
Our reading contained multiple interesting PFL papers so we think this directions is very promising and worth working on.

\subsubsection{Complementary Components \& Integrations}
The FLOps project has to be currently cloned, configured slightly and launched to work with it.
FLOps is a collection of components and relies upon its orchestrator and is heavily supported by its CLI.
This CLI could be further improved to automate even more aspects of FLOps and its orchestrator dependency.
Ideally the CLI could be installed via pip and with a single command the entire needed dependencies, repositories and components of FLOps and its orchestrator would be installed and launched to make working with these tools as easy and quick as possible.
Realizing this goal is not hard because the CLI already has many necessary functionalities that simply needs to be expanded and combined.
Besides this FLOps should be properly integrated into Oakestra's new addon marketplace and augmented via its hooks to react to events.
Once this is realized FLOps will be even more responsive and easier for users to work with.

