\subsection{Fundamentally Different Projects}

\subsubsection{Longer Training Rounds with more Learners}

\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.85\paperwidth]{eval_2_simple_large_cpu_and_mem.png}
        \caption{Experiment 2: CPU \& Memory}
        \label{fig:eval_2_cpu_mem}
    \end{adjustwidth}
\end{figure}

Figure \ref{fig:eval_2_cpu_mem} shows the utilization of CPU and memory of experiment (2).
Because this project configuration uses more learners and training rounds it is trivial that the FL training stage will last longer.
This is also visible in the stage durations of Figure \ref{fig:eval_2_simplest_stage_durations} where FL training is now the longest stage.
Note that this larger example is still relatively small compared to ML/FL training periods that take multiple hours or days to complete.
As a result the CPU utilization during training is less spread than for (1) but concentrates in the high 90-100\% range.
Memory shows a similar shift from low 60s to los 70s.
The disk space and net-IO stay similar to (1).
The resulting accuracies are now all in the mid 80s instead of below it as in (1), thus the longer training period led to better results.

\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.90\paperwidth]{eval_2_simple_large_stage_durations.png}
        \caption{Experiment 2: Stage Durations}
        \label{fig:eval_2_simplest_stage_durations}
    \end{adjustwidth}
\end{figure}

\subsubsection{Different ML Repository, Framework, and Dataset}

\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.85\paperwidth]{eval_5_simple_pytorch_cpu_and_mem.png}
        \caption{Experiment 5: CPU \& Memory}
        \label{fig:eval_5_cpu_mem}
    \end{adjustwidth}
\end{figure}

Figure \ref{fig:eval_5_cpu_mem} the CPU and memory utilization of experiment (5).
The first obvious difference to (1) is the almost threefold project duration, especially due to build times.
Pytorch is a more heavy-wheight ML library than Scikit-learn thus configuring its larger dependencies takes longer.
Figure \ref{fig:eval_5_simplest_stage_durations} depicts (5)'s stage durations.
(5)'s CPU behaviour is similar to (1).
The FL training in (5) requires less memory (mid 50s) than in (1) (low 60s).
\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.90\paperwidth]{eval_5_simple_pytorch_stage_durations.png}
        \caption{Experiment 5: Stage Durations}
        \label{fig:eval_5_simplest_stage_durations}
    \end{adjustwidth}
\end{figure}

\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.85\paperwidth]{eval_5_simple_pytorch_disk_space.png}
        \caption{Experiment 5: Disk Space}
        \label{fig:eval_5_disk_space}
    \end{adjustwidth}
\end{figure}

Figure \ref{fig:eval_5_disk_space} shows the remarkable influence of the chosen ML framework and its dependencies for FLOps.
Compared to the relatively light-weight Sklearn base-case example with a total used disk space increase of 14 GB, this simple pytorch examples takes up approximately 35 GB in the end.
During build times where dependencies are pulled the peak extra disk-space utilization reaches 60 GB.
We remind that the base-image for Pytorch is already 7.5 GB in size, without all additional user, MLOps, and FL dependencies.
These heavy dependencies are also visible in the network-IO in Figure \ref{fig:eval_5_net_io}.
The garbage collection is strongly present and visible in this case.
The tiny number of learners and training rounds lead to a small accuracy of 40\% of the final models.

\begin{figure}[H]
    \begin{adjustwidth}{-0.2\paperwidth}{-0.2\paperwidth}
        \centering
        \includegraphics[width=0.85\paperwidth]{eval_5_simple_pytorch_net_io.png}
        \caption{Experiment 5: Network IO}
        \label{fig:eval_5_net_io}
    \end{adjustwidth}
\end{figure}

As a result these experiments prove that FLOps is capable of handling different FL training configurations using various ML frameworks, repositories, and datasets.
They also unveil that using popular classic ML libraries might be unrealistic for resource-constrained edge devices.
Thus libraries that are dedicated for restricted devices should be analyzed and tried out as future work.