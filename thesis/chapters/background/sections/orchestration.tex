\section{Orchestration}

Modern orchestration and containerization has been rapidly evolving for the last decade.
To avoid issues due to different environments and dependencies containers arose as a lightweight alterative to heavier virtual machines.
The most popular containerization software Docker was released in 2013.
Multiple individual containers quickly got grouped together to separate concerns and allow decoupled workloads.
This became especially useful and popular with the rise of micro service architectures.
Handling various containers at the same time is challenging.
Techniques such as Docker Compose made this easier, however dynamically scaling and handling failing containers was still difficult.
These swarm of containers needed additional management tooling for orchestrating these dynamic processes.
The most prominent orchestrator to date, Kubernetes, was released in 2014.
Since then many new endevours formed to unify and streamline future developents in the field.
Examples include the Open Container Initiate or the Moby Project.
Many different alternatives and competitors to Docker and Kubernetes have developed over the years.

\subsection{ML Containerization \& Orchestration}

Performing ML in containers is an effective approach.
The goal of FLOps is to automate and orchestrate FL workloads on distributed heterogeneous machines.
It is crucial to confirm that doing FL/ML via orchestrated containers is viable and that FL processes will not suffer from bottlenecks and problems due to running inside of containers.
In 2017, Xu et al. \cite{paper:dl_via_docker} evaluated deep learning (DL) tool performance in docker containers.
They analyzed CPU, GPU and I/O utilizations.
They found that DL works equally well in containers as running it directly on the host machines.

Containerized ML is widely used today.
In 2022, Openja et al. \cite{paper:ml_practices_on_docker} analyzed more than 400 different open-source ML-based projects that use docker containers.
These containers are used for various tasks.
A popular application of containerization technologies in ML is to streamline and improve deployment efforts.
This work demonstrates that nowadays containers are used in all ML-related tasks.

Orchestration efforts are optional for classic ML but essential for FL.
As discussed in \ref{section:federated_learning} classic centralized ML can be trained on a single powerful machine.
FL, especially Cross-Device edge FL, can involve massive dynamic numbers of devices.
Managing all these components is challenging.
Training an ML model requires more than just the ML code and model.
The device performing the training needs to support the necessary environment and dependency requirements.
The setup and configuration of devices is a limiting and error-prone task.
To avoid potential issues and to allow as many devices to participate as possible FL should use containers. 


\subsection{Oakestra}

FLOps is primarily focused on enabling practical FL on real machines.
The main target group for cross-device FL are heterogeneous edge devices.
Kubernetes is not designed for the edge but for homogeneous, resource-rich cloud environments, whereas Oakestra is designed for the edge and is able to outperform Kubernetes there \cite{paper:oakestra_usenix}.

Oakestra is a hierarchical, open-source orchestrator for edge computing.
It has a lightweight and flexible code base.
It features many novel techniques, such as semantic overlay networking for efficient service communication.

\begin{figure}[!ht]
    % \begin{adjustwidth}{-\oddsidemargin-1in}{-\rightmargin}
    \begin{adjustwidth}{-0.1\paperwidth}{-0.1\paperwidth}
        \centering
        \includegraphics[width=0.9\paperwidth]{oakestra_simple_architecture.png}
        \caption{Simplified Oakestra Architecture}
        \label{fig:simple_oakestra_architecture}
    \end{adjustwidth}
\end{figure}

Figure \ref{fig:simple_oakestra_architecture} shows a simplified architecture of Oakestra.
This unique federated three-tiered structure allows for scalable delegate task scheduling and execution.
Instead of a single control plane as in Kubernetes, Oakestra distributes its control among
the root and cluster orchestrators.
Oakestra is specialized to handle resource constrained, heterogeneous, devices that are spread across various geographical areas.
Different infrastructure providers can have their own isolated cluster and cluster orchestrator.
Cluster orchestrators only have access to detailed information about worker from their own cluster.
The metrics they share with the root are distilled and no longer contain sensitive individual meta data.
This is an ideal environment for FL because this layout supports privacy on an structural level.
FLOps uses Oakestra to orchestrate its components.

