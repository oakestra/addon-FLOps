\begin{changemargin}{-2cm}{0cm}
    \begin{tabular}{|c||m{0.4\paperwidth}|m{0.4\paperwidth}|}
        \hline
            ID & Contributions & Limitations Future Work \\
        \hline
            \cite{paper:refl_resource_efficient_fl}
            &
            Proposed a novel selection and staleness-aware aggregation strategy.
            They highlight resource wastage and the impact of stragglers.
            Introduce a smart participation selection based on learner availability.
            &
            They do not yet consider privacy or security.
            Their evaluations are based on classic datasets (MNIST, CIFAR-10) which might not reflect real non-IID data.
            They assumes homogeneous resources.
            They use a simple linear regression model for availability prediction.
            More sophisticated alternatives exist.
            They want to use more factors for availability prediction like battery level, bandwidth, and user preferences.
        \\
        \hline
            \cite{paper:cluster_based_secure_aggregation_for_fl}
            &
            Proposes a novel cluster-based secure aggregation strategy for diverse nodes.
            Clustering based on processing score \& GPS information/latency.
            Leads to better throughput.
            Reduces false-positive dropouts.
            Introduces a novel additive sharing-based masking scheme that is robust against dropouts.
            &
            Assumes that all participants are honest - does not evaluated what happens with malicious users.
            Server might become a bottleneck - can be resolved by using HFL (with cluster heads).
            Only image classification was checked, no other ML task.
        \\
        \hline
            \cite{paper:privacy_preserving_deep_fl_for_coop_hierarchical_caching_in_fog_computing}
            &
            They propose an FL caching scheme including novel algorithms and architecture.
            They utilize an AI training model that considers user history.
            &
            They do not provide a convergence analysis.
            They want to investigate blockchain-empowered FL to further improve security/privacy.
        \\
        \hline
            \cite{paper:model_pretraining_and_initialization_for_fl}
            &
            They investigated the impact of pre-training and initializing
            ML models for FL compared to the common random approach.
            They found that one should prefer pre-trained models.
            &
            It is challenging to get a pre-trained model if the necessary data is not available or private.
            Using pre-trained models can lead to biases.
            They only consider a specific (warm-start) initialization strategy.
        \\
        \hline
            \cite{paper:decentralized_edge_intelligence_dynamic_resource_allocation_framework_hfl}
            &
            Proposed a novel incentive/resource-based allocation schema that utilizes game theory.
            Learners with more data are more valuable and they can compete for higher participation rewards.
            Multiple model owners compete for cluster heads with most data.
            &
            They want to consider social network effects and their impact on cluster selection decisions of workers.
            Account for existing evil workers.
        \\
        \hline
            \cite{paper:fedat_high_performance_communication_efficient_fl_with_asynch_tiers}
            &
            Uses asynchronous tiers.
            Synergy of asynchronous \& synchronous FL
            Able to handle stragglers.
            &
            The tiers all update the server individually.
            This could further be improved via HFL with intermediate cluster heads to do the aggregation
            Additional security could be applied at these cluster heads.
        \\
        \hline
    \end{tabular}
    \captionof{table}{A Subset of the FL Papers considered for FLOps} 
    \label{table:main_fl_research_table}
\end{changemargin}
