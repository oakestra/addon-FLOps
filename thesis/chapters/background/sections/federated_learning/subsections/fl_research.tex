\subsection{FL Research}\label{subsection:fl_research}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fl_documents_research.png}
    \caption{Evolution of FL Publications}
    \label{fig:fl_documents_research}
\end{figure}

Figure \ref{fig:fl_documents_research} shows the exponential growth of FL documents
since 2016. (This data comes from searching for "federated learning" in article title, abstract, or keywords via Scopus \cite{scopus_homepage}.)
Note that we based the idea for this graph on \cite{thesis:tum_fl_framework_comparison},
and we used a different query with the latest available data.

Before we started working on FLOps, we wanted to find research gaps in the fields of 
ML at the edge, specifically FL.
In total, we have read and examined 47 papers in detail, with 26 papers focusing on FL. 
Additionally, we consulted several articles,
joined and participated in discussion forums,
and completed a couple of paid courses \cite{udemy_homepage}.
Discussing each paper in detail would heavily bloat this thesis.
We present key and meta-findings instead.
During our reading, we created and incrementally updated a database in which we noted the specific properties of each paper.
These properties include one or multiple categories in which the paper fits in, the initial problems or challenges the authors tried to resolve,
their contributions, results, limitations, and envisioned future work.
We also noted down what ML or FL frameworks or libraries they used.
We based these properties on our subjective analysis instead of extracting them verbatim from the paper.

\begin{figure}[p]
    \input{tables/main_fl_research_table.tex}
\end{figure}

Table \ref{table:main_fl_research_table} depicts a subset of the FL papers we analyzed.
It shows the documented contributions, limitations, and future work properties.
We explicitly decided to use an abbreviated format instead of verbose sentences
to optimize the limited space.
One can inspect the remaining FL papers in the appendix \ref{appendix:fl_research}.
These tables should provide a good impression of the individual papers we examined.
We look for patterns and trends to better understand the research field of FL as a whole.
We utilize the documented properties for this.

Figure \ref{fig:fl_research_categories} shows the different found categories and their distribution.
Most of our papers were focused on performance, trying new concepts, finding best practices,
and exploring different FL architectures.
Only two papers focused on deployment and orchestration.
\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_research_categories.png}
    \caption{FL Paper Categories}
    \label{fig:fl_research_categories}

    \includegraphics[width=1.0\textwidth]{fl_research_problem_challenge.png}
    \caption{Targeted Problems \& Challenges of FL Papers}
    \label{fig:fl_research_problem_challenge}
\end{figure}
A similar trend can be seen in figure \ref{fig:fl_research_problem_challenge}.
The primary focus is on investigating new concepts or improving existing bottlenecks
in terms of performance, scalability, and complexity.
We point out that several papers aimed to narrow the gap between industry and research or
to make FL easier to use.
This ease of use seems to focus on improving already configured and working FL setups.

The main contributions seen in figure \ref{fig:fl_research_contributions} strengthen this assumption.
This chart is dominated by mathematical and conceptual proofs that novel
architectures and algorithms work as proposed.
Contributions do not seem to focus on improving the initial setup, deployment, and configuration processes.
\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_research_contributions.png}
    \caption{FL Paper Contributions}
    \label{fig:fl_research_contributions}

    \includegraphics[width=0.9\textwidth]{fl_research_achieved_results.png}
    \caption{Achieved Results of FL Papers}
    \label{fig:fl_research_achieved_results}
\end{figure}
The results achieved mirror these finding.
Figure \ref{fig:fl_research_achieved_results} shows that these contributions
lead to better efficiencies in terms of speed, resource utilization, training results,
and handling of heterogeneous data.
Note that we based these properties on the results and contributions the authors mentioned themselves
and on our conclusions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fl_research_limitations_future_work.png}
    \caption{Limitations \& Future Work of FL Papers}
    \label{fig:fl_research_limitations_future_work}
\end{figure}

Figure \ref{fig:fl_research_limitations_future_work} reflects our perception.
If specified, the focal point is on improving privacy and security, further performance optimizations, or adding support for more ML use cases.
Even the future focus is not on optimizing accessibility, usability, or the mentioned initial vital steps.

Because we assigned these properties subjectively and our paper sample size is relatively small,
we compare our findings so far with the total number of published works about FL.
We use the same method to gather the data as for \ref{fig:fl_documents_research}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_publications_compared.png}
    \caption{Evolution of FL Publications based on Keywords}
    \label{fig:fl_publications_compared}
\end{figure}
Figure \ref{fig:fl_publications_compared} shows how many works have been published in FL with
specific keywords that match our custom categories.
The global results paint a similar picture as our samples.
The most popular topics in FL are related to privacy/security, performance,
or algorithms.
Only a tiny portion of FL papers focus on usability, automation, orchestration,
or other initial steps.

It seems that researchers assume others to already have working FL environments
and motivate their readers to optimize them based on their findings
instead of replicating and configuring such an FL setup initially.
One can also see these tendencies when inspecting the ML and FL frameworks and libraries
the authors mentioned they used in our examined papers.
Figure \ref{fig:fl_research_ml_frameworks} shows that most authors
did not explicitly state what ML framework or library they used for their work.
Many researchers used Pytorch and TensorFlow.
\begin{figure}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{fl_research_ml_frameworks.png}
    \caption{Distribution of mentioned ML Frameworks in FL Papers}
    \label{fig:fl_research_ml_frameworks}

    \includegraphics[width=0.9\textwidth]{fl_research_fl_frameworks.png}
    \caption{Distribution of mentioned FL Frameworks in FL Papers}
    \label{fig:fl_research_fl_frameworks}
\end{figure}
Figure \ref{fig:fl_research_fl_frameworks} shows that FL researchers
rarely mention what FL frameworks they use for their work.
It is much more common for authors to mention what ML 
framework they used than what FL framework they used.

Possible reasons for this might be that ML as a field is a lot older, more sophisticated,
widespread, and established.
The same applies to ML frameworks.
On the other hand, FL is a very young subfield of ML research.
FL frameworks are still in their early stages.
Therefore, FL researchers might be using FL frameworks, but due to the framework's immaturity,
the researchers might not deem it important to explicitly point out that they used them.
Another possible explanation is that FL researchers are experts in FL and
can set up and configure FL from the ground up on their own.
Either way, this lack of transparency makes reproducing or extending
their work challenging, if not infeasible.

These gaps in FL research motivated the creation of FLOps.
