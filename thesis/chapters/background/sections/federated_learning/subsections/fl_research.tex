% research has been focused on optimizing FL algos
% comming up with new algos
% optimizing bottlenecks
%
% briefly mention the following aspects to highlight that FL is a complex topic with lots of things people are working on
% "Client-Side Local Training" - Computation, Memory, Energy, Network
% mention Security/Privacy
% https://www.notion.so/oakestra-team/Chapter-10-Local-Training-Scalability-of-FL-Systems-c0f945e2593e48ea9bdbb71c1d94d091?pvs=4
%
% Lack of papers discussing how to set-up, manage, orchestrate FL in the first place
% rarely talk how they set up their experiments
% rarely mention what they use for FL or even for ML
% very hard - intransparent - to reproduce, etc.
%
%Mention a hand-ful of papers here and there what they are looking into to signal that hey I did my research and reading
\subsection{FL Research}\label{subsection:fl_research}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fl_documents_research.png}
    \caption{Evolution of FL Publications}
    \label{fig:fl_documents_research}
\end{figure}

Figure \ref{fig:fl_documents_research} shows the exponential growth of FL documents
since 2016. (This data comes from searching for "federated learning" in article title, abstract or keywords via Scopus \cite{scopus_homepage}.)
Note that we based the idea for this graph on \cite{thesis:tum_fl_framework_comparison}
and we used a different query with the latest available data.

Before we started working on FLOps we wanted to find research gaps in the fields of 
ML at the edge, specifically FL.
In total we have read and examined 47 papers in detail. 
26 papers were focused on FL. 
Additionally, we consulted several articles,
joined and participated in discussion forums,
and completed a couple of payed courses \cite{udemy_homepage}.

TODO TODO 
I think this is a great idea, the graphs and table
I would only feature the most important once in the graph and table.
The rest should be placed in the appendix (will still provide from huge reference list)

% NOTE: I am aware that this is very hacky, but I tried to get this proper for a couple of hours with no success.
\begin{changemargin}{-2cm}{0cm} 
    \begin{tabular}{|c||m{0.4\paperwidth}|m{0.4\paperwidth}|}
        \hline
            ID & Contributions & Limitations Future Work \\
        \hline
            \cite{HPFL_over_massive_mobile_edge_computing_networks}
            & 
            Likely the first paper to combine PFL with HFL in a three-tiered structure.
            Proves mathematically that this approach works and converges.
            Includes many interesting insights regarding the HPFL.
            &
            -
        \\
        \hline
            \cite{paper:cluster_based_secure_aggregation_for_fl}
            &
            Proposes a novel cluster-based secure aggregation strategy for diverse nodes.
            Clustering based on processing score \& GPS information/latency.
            Leads to better throughput.
            Reduces false-positive dropouts.
            Introduces a novel additive sharing-based masking scheme that is robust against dropouts.
            &
            Assumes that all participants are honest - does not evaluated what happens with malicious users.
            Server might become a bottleneck - can be resolved by using HFL (with cluster heads).
            Only image classification was checked, no other ML task.
        \\
        \hline
            \cite{paper:fedat_high_performance_communication_efficient_fl_with_asynch_tiers}
            &
            Uses asynchronous tiers.
            Synergy of asynchronous \& synchronous FL
            Able to handle stragglers.
            &
            The tiers all update the server individually.
            This could further be improved via HFL with intermediate cluster heads to do the aggregation
            Additional security could be applied at these cluster heads.
        \\
        \hline
            \cite{paper:hfl_with_privacy}
            &
            Analyzed benefits of HFL for security.
            Proposes HFL secure aggregation method
            and hierarchical DP.
            &
            The number of (online) clients per zone has to be small.
            Investigate further privacy improvements.
        \\
        \hline
            \cite{paper:edge_fl_via_mqtt_and_oma_lightweight_m2m}
            &
            Proposes novel approach of finding and sharing information between FL components and discovering learners.
            Uses MQTT with semantic URIs that represent the properties of the clients, including their resources.
            &
            It is a very short paper.
            The experiments are only simulated.
            They did not extensively compare their approach with other classic or novel approaches.
        \\
        \hline
            \cite{paper:decentralized_edge_intelligence_dynamic_resource_allocation_framework_hfl}
            &
            Proposed a novel incentive/resource-based allocation schema that utilizes game theory.
            Learners with more data are more valuable and they can compete for higher participation rewards.
            Multiple model owners compete for cluster heads with most data.
            &
            They want to consider social network effects and their impact on cluster selection decisions of workers.
            Account for existing evil workers.
        \\
        \hline
            \cite{paper:hfl_with_momentum_acceleration_in_multi_tier_networks}
            &
            Accelerated and improved FL training and the aggregation algorithm.
            They use a hierarchical structure and ML momentum.
            &
            They do not cover security/privacy aspects or the impact on the network.
        \\
        \hline
            \cite{paper:efficient_privacy_preserving_ml_in_hierarchical_distributed_systems}
            &
            They improvement the efficiency of privacy-preserving ML techniques for hierarchically distributed structures.
            They considered different data partitions and distributions, such as vertical and non-IID.
            &
            Written in 2019.
            Many other, newer papers have investigated HFL security/privacy further.
        \\
        \hline
            \cite{paper:refl_resource_efficient_fl}
            &
            Proposed a novel selection and staleness-aware aggregation strategy.
            They highlight resource wastage and the impact of stragglers.
            Introduce a smart participation selection based on learner availability.
            &
            They do not yet consider privacy or security.
            Their evaluations are based on classic datasets (MNIST, CIFAR-10) which might not reflect real non-IID data.
            They assumes homogeneous resources.
            They use a simple linear regression model for availability prediction.
            More sophisticated alternatives exist.
            They want to use more factors for availability prediction like battery level, bandwidth, and user preferences.
        \\
        \hline
            \cite{paper:privacy_preserving_deep_fl_for_coop_hierarchical_caching_in_fog_computing}
            &
            They propose an FL caching scheme including novel algorithms and architecture.
            They utilize an AI training model that considers user history.
            &
            They do not provide a convergence analysis.
            They want to investigate blockchain-empowered FL to further improve security/privacy.
        \\
        \hline
            \cite{paper:adaptive_fl_for_resource_constrained_edge}
            &
            They investigated effects of different global/local update frequencies.
            They proposed a new algorithm to determine global aggregation frequency instead of using the common static one.
            &
            They want to investigate diverse resource usage.
        \\
        \hline
            \cite{paper:scaling_fl_for_fine_tuning_llms}
            &
            Studies how LLMs behave in FL when using different number of learners.
            &
            Due to its proof-of-concept nature this work only features trivial experiments that do not lead to many new insights.
        \\
        \hline
            \cite{paper:fl_inference_anytime_anywhere}
            &
            They combine FL with transfer-learning on Transformers.
            Introduces a parameter efficient (PE) learning method to adapt pre-trained Transformer Foundation Models (FMs) in FL.
            Their novel PE adapter modulates all layers of pre-trained Transformers,
            that enables flexible early predictions.
            &
            -
        \\
        \hline
            \cite{paper:tackling_objective_inconsistency_problem_in_heterogeneous_fl}
            &
            They reason about drift that occurs due to different learner speeds
            and propose ways of eliminating that drift.
            &
            This work does not consider hierarchical structures,
            clusters/tiers, nor privacy/security.
        \\
        \hline
            \cite{paper:model_pruning_for_edge_fl}
            &
            They introduce distributed adaptive FL model pruning.
            &
            They did not consider privacy/security.
            They want to investigate further optimizations including GPUs.
        \\
        \hline
            \cite{paper:adaptive_exper_models_for_pfl}
            &
            They improved an existing PFL algorithm that used clustered models (but discarded all but one in the end)
            The propose to use these cluster models as MoE to improve performance.
            &
            todo
        \\
        \hline
            todo
            &
            todo
            &
            todo
        \\
        \hline
    \end{tabular}
\end{changemargin}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_categories.png}
%     \caption{TODO}
%     \label{fig:fl_research_categories}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_contributions.png}
%     \caption{TODO}
%     \label{fig:fl_research_contributions}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_problem_challenge.png}
%     \caption{TODO}
%     \label{fig:fl_research_problem_challenge}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_limitations_future_work.png}
%     \caption{TODO}
%     \label{fig:fl_research_limitations_future_work}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_achieved_results.png}
%     \caption{TODO}
%     \label{fig:fl_research_achieved_results}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_ml_frameworks.png}
%     \caption{TODO}
%     \label{fig:fl_research_ml_frameworks}
% \end{figure}

% TODO

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fl_research_fl_frameworks.png}
%     \caption{TODO}
%     \label{fig:fl_research_fl_frameworks}
% \end{figure}

% TODO