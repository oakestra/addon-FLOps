
\subsection{FL Research}\label{subsection:fl_research}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fl_documents_research.png}
    \caption{Evolution of FL Publications}
    \label{fig:fl_documents_research}
\end{figure}

Figure \ref{fig:fl_documents_research} shows the exponential growth of FL documents since 2016.
(This data comes from searching for "federated learning" in article title, abstract, or keywords via Scopus \cite{scopus_homepage}.)
The idea for this graph is based on \cite{thesis:tum_fl_framework_comparison}.
Graph \ref{fig:fl_documents_research} uses a different query with the latest available data.

Before creating FLOps, we looked for research gaps in the fields of ML at the edge, specifically FL.
We have read and examined 47 papers in detail, with 26 papers focusing on FL. 
Additionally, we consulted several articles, joined and participated in discussion forums, and completed a couple of paid courses.
Discussing each paper in detail would heavily bloat this thesis.
This subsection presents key and meta-findings instead.
While working through the material, we created and incrementally updated a database in which we noted specific properties of each paper.
These properties include one or multiple categories in which the paper fits in.
Additional properties include the initial problems or challenges the authors tried to resolve, their contributions, results, limitations, and envisioned future work.
We also noted what ML or FL frameworks or libraries they claimed to use.

\begin{figure}[p]
    \input{tables/fl_research_table_1.tex}
\end{figure}

\begin{figure}[p]
    \input{tables/fl_research_table_2.tex}
\end{figure}

\begin{figure}[p]
    \input{tables/fl_research_table_3.tex}
\end{figure}

Tables \ref{table:fl_research_table_1}, \ref{table:fl_research_table_2}, and \ref{table:fl_research_table_3} depict our analyzed FL papers.
They present the documented contributions, limitations, and future work properties.
When there is no content (-) in the "Limitations \& Future Work" column that means that the authors did not mention any explicitly and that we did not notice anything specifically.
These tables provide a good impression of the examined FL papers.
Patterns and trends can be extracted from these papers based on the documented properties.

Patterns and trends help to better understand the research field of FL as a whole.
Figure \ref{fig:fl_research_categories} shows the different categories and their distribution.
Most examined papers focused on performance, trying new concepts, finding best practices, and exploring different FL architectures.
Only two papers focused on deployment and orchestration.

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_research_categories.png}
    \caption{FL Paper Categories}
    \label{fig:fl_research_categories}

    \includegraphics[width=1.0\textwidth]{fl_research_problem_challenge.png}
    \caption{Targeted Problems \& Challenges of FL Papers}
    \label{fig:fl_research_problem_challenge}
\end{figure}

Figure \ref{fig:fl_research_problem_challenge} reveals a similar trend.
The primary focus is on investigating new concepts or improving existing performance, scalability, and complexity bottlenecks.
Several papers have aimed to narrow the gap between industry and research or make FL easier to use.
This ease of use seems to focus on improving already configured and working FL setups.
The main contributions seen in Figure \ref{fig:fl_research_contributions} strengthen this assumption.
Mathematical and conceptual proofs dominate this chart.
They prove that novel architectures and algorithms work as proposed.
Contributions do not seem to focus on improving the initial setup, deployment, and configuration processes.

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_research_contributions.png}
    \caption{FL Paper Contributions}
    \label{fig:fl_research_contributions}

    \includegraphics[width=0.9\textwidth]{fl_research_achieved_results.png}
    \caption{Achieved Results of FL Papers}
    \label{fig:fl_research_achieved_results}
\end{figure}

The achieved results mirror previous findings.
Figure \ref{fig:fl_research_achieved_results} shows that these contributions lead to more efficient FL.
Improved aspects include speed, resource utilization, training results, and handling of heterogeneous data.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fl_research_limitations_future_work.png}
    \caption{Limitations \& Future Work of FL Papers}
    \label{fig:fl_research_limitations_future_work}
\end{figure}

Figure \ref{fig:fl_research_limitations_future_work} reflects this perception.
If specified, the focal point is on improving privacy and security, further performance optimizations, or adding support for more ML use cases.
Even the future focus is not on optimizing accessibility, usability, or the mentioned initial vital steps.
These documented properties might be biased, and the inspected sample size of papers is relatively small.
To improve confidence in these findings, we compare them with the total number of published works about FL.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{fl_publications_compared.png}
    \caption{Evolution of FL Publications based on Keywords}
    \label{fig:fl_publications_compared}
\end{figure}

Figure \ref{fig:fl_publications_compared} depicts how many works have been published in FL with specific keywords that match our custom categories.
We applied the same method to gather the data as for \ref{fig:fl_documents_research}.
The global results paint a similar picture as our samples.
The most popular topics in FL are related to privacy/security, performance, or algorithms.
Only a tiny portion of FL papers focus on usability, automation, orchestration, or other initial steps.

It seems that researchers assume others to already have working FL environments.
Furthermore, they seem to motivate their readers to optimize these setups based on their findings instead of replicating and configuring such an FL setup initially.
These tendencies are visible when inspecting the ML and FL frameworks and libraries the authors mentioned they used.
The following figure is again based on our examined papers.
Figure \ref{fig:fl_research_ml_frameworks} shows that most authors did not explicitly state what ML framework or library they used for their work.
Many researchers used Pytorch and TensorFlow.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fl_research_ml_frameworks.png}
    \caption{Distribution of mentioned ML Frameworks in FL Papers}
    \label{fig:fl_research_ml_frameworks}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fl_research_fl_frameworks.png}
    \caption{Distribution of mentioned FL Frameworks in FL Papers}
    \label{fig:fl_research_fl_frameworks}
\end{figure}

Figure \ref{fig:fl_research_fl_frameworks} shows that FL researchers rarely mention what FL frameworks they use for their work.
It is much more common for authors to mention what ML framework they used than what FL framework they used.
Possible reasons for this might be that ML as a field is a lot older, more sophisticated, widespread, and established.
The same applies to ML frameworks.
On the other hand, FL is a very young subfield of ML research.
FL frameworks are still in their early stages.
FL researchers might be using FL frameworks.
However, due to the framework's immaturity, the researchers might not deem it important to explicitly point out that they used them.
Another possible explanation is that FL researchers are experts in FL and can set up and configure FL from the ground up.
Either way, this lack of transparency makes reproducing or extending their work challenging, if not infeasible.
These gaps in FL research motivated the creation of FLOps.