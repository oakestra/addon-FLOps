
\subsection{FL Basics}

Note that we assume the reader to be familiar with basic machine learning concepts.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{classic_ml_training.png}
    \caption{Centralized ML Model Training}
    \label{fig:classic_ml_training}
\end{figure}
Figure \ref{fig:classic_ml_training} depicts the classic centralized ML model training process.
Starting from (1), where clients have their data (D) and
the server hosts the untrained (gray) ML model (M).
In (2), the clients send their data to the server.
The server can now train the model using data from both clients.
(3) depicts the final state after training.
(The pink/purple model color represents that both data sources, red and blue,
have been used during training.)
The client data remains on the server and is exposed to potential exploitation.

As discussed in the introductory chapter, the centralized approach often leads to
privacy breaches. FL was introduced to use this lucrative sensitive data
on client devices for training ML models while keeping that data private and 
complying with laws and regulations.
Many different algorithms and strategies exist for FL.
We focus on the widely used base-case/classic FL algorithm FederatedAveraging (FedAvg)
proposed as part of the original FL paper \cite{paper:original_fl}.

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{basic_fl_intro.png}
    \caption{Basic Federated Learning}
    \label{fig:basic_fl_intro}
\end{figure}
Figure \ref{fig:basic_fl_intro} shows the basic FL training loop.
Note that the number of learners can vary.
This and the following figures mainly represent such groups
only via two members, to optimize page space.
The first differences are the component names.
In FL, the server is frequently referred to as an \textbf{aggregator}
and coordinates the FL processes.
Clients are called \textbf{learners}.
Note that using the terms server and clients in FL is still common.
We prefer aggregators and learners because it highlights that these are FL components.
This naming choice is also used in FLOps and helps with comprehension
because FLOps uses a manifold of components, including non-FL servers and clients.
Another difference is that all components must know and possess the ML model locally.
They also need to set up their environment for training properly.

Initially, at (1), all models are untrained.
At (2), the aggregator starts the first FL training cycle by telling the learners
to start their local training.
The local training rounds (epochs) are completed at (3).
(The 'M's are now colored.)
As a reminder, one can split up ML models into two parts.
One part is (usually) a static lightweight model architecture
that includes layer specification (in DNNs), training configuration,
hyperparameters like learning step sizes, and what loss type or
activation function to use.
Model weights and biases are the dynamic components of an ML model.
A model without them is not useful because weights and biases
are what get trained and allow the model to fulfill its intended use,
such as prediction, inference, or generation tasks.
These weights and biases are the major contributors to
a trained model's overall size (space utilization).
Because the model architecture is static in classic ML/FL,
one can transmit the weights and biases between aggregators and learners
instead of the entire trained model.
We call everything that gets sent between the learners and aggregators
(model) \textbf{parameters} and depict it with (P).

In (4), the learners have extracted their model parameters and sent them to the aggregator.
The aggregator now has access to these parameters but not
the sensitive data used to train them.
That is how FL can profit from sensitive data while maintaining its privacy.
Note that there are still attack vectors that allow exposing sensitive client information
by abusing this parameter-based aggregation process.
We briefly discuss this and other FL security aspects later on.

In (5), the server aggregates these collected parameters into
new global parameters, which the aggregator applies to its model instance.
This aggregation process is also called model fusion.
Because learners can be heterogeneous and possess varying amounts of data,
some learner updates might be more impactful than others.
To respect this circumstance, learners typically also send the number
of data samples, they used for training, to the aggregator.
That way, the aggregator can prioritize its received updates proportionally.
Otherwise, in classic FL aggregation, the mean of the parameters is used for the global model.
The result is a \textbf{global model} that was trained for one FL cycle.

In (6), the aggregator sends its global parameters back to the learners.
The learners apply these parameters to their local model instance
to make it identical to the aggregator's global model,
thus discarding their locally trained parameters.
Now, the FL training loop could terminate, and the learners or servers
could use their global model copy for inference, or
as depicted in (6), another FL training cycle begins.
There can be arbitrarily many FL cycles, similar to conventional training rounds
in classic ML. FL training will have to stop,
due to time/resource constraints or reaching a satisfying performance.
Otherwise, the accuracy and loss will worsen due to overfitting,
assuming the available training data is finite and unchanging.
