\subsection{FL Frameworks \& Libraries}\label{subsection:fl_frameworks_and_libraries}

To better comprehend why so many researchers did not specify or use FL frameworks, we examine
the current landscape of available FL frameworks.
We will keep this discussion short because
Saidani already analyzed and evaluated FL frameworks in great detail
in his master's thesis \cite{thesis:tum_fl_framework_comparison} from 2023.
He examined FL libraries, frameworks, and benchmarks.
He found that many FL tools exist for specific niche use cases and architectures.
This is contrary to the opinions of his questioned FL practitioners and experts, who
expect FL libraries and frameworks to focus on basic FL features,
such as communication, aggregator-learner orchestration, security, and data aggregation.
Saidani found that many libraries and frameworks, most of which are not production-ready,
are still in an experimental research state.

To reduce complexity, he focused on the five most promising open-source frameworks.
For a framework to be allegeable, it had to fulfill 2/3 of the following criteria.
It needed more than one thousand starts and 350 forks on GitHub.
The interviewed experts had to mention it.
The framework had to support all major operation systems.
Because FL is rapidly evolving, we updated his findings and expanded upon them by including
the last version released, the last commit pushed, and the number of open issues in the repository.

\input{tables/fl_framework_comparison_table.tex}

Table \ref{table:updated_fl_framework_comparison} shows our updated FL Framework comparison.
Note that we took these stats on 16.08.2024.
These FL frameworks are in active development. 
Only FedML has not been updated for several months now.

Saidani's main original contribution was a novel FL benchmarking suite called FMLB (Federated Machine Learning Benchmark).
He developed it to evaluate and compare the mentioned FL frameworks efficiently.
His previous analysis and summary of existing frameworks were sound and helpful.
However, we are critical of his evaluation results, especially the poor performance of Flower surprised us.
We tried to replicate his experiments, but his provided code \cite{tum_fl_framework_thesis_github}
lacks instructions on how to set up this benchmark application.

We simulated the experiments with the latest official flower version of that time,
and made sure to stick as close as possible to the same experimental setup and configuration.
Our findings show very different results.
Flower manages to solve the experiment quickly and efficiently.
Our results match the verdicts of other works comparing FL frameworks, such as \cite{comparative_analysis_of_fl_frameworks} or \cite{comprehensive_study_fl_frameworks_degree_project}.
\cite{comparative_analysis_of_fl_frameworks} is the latest work that compares FL frameworks that we considered,
and its verdict is that Flower even outperforms all its competition.

We decided to use Flower as the FL framework for FLOps.