\subsection{Supplementary FL Concepts}

This subsection explores essential supplementary FL concepts to understand the field better.

\subsubsection{FL compared to Distributed Learning}

At first glance, FL seems similar to Distributed Learning (DL).
Both get used for computationally expensive large ML tasks.
Computations get distributed among many weaker machines that train individually.
This approach increases convergence times and avoids the need for powerful devices. 
Afterward, a global model gets aggregated at the server.
Regarding their differences, the quantity and distribution of training data can be very diverse in FL and might remain unknown throughout training.
FL only uses the data that the learners offer.
DL starts with full centralized access and control to all data before splitting it up among its fixed and predefined clients \cite{book:fl}.
Thus, DL does not support the privacy concerns because it has total oversight and control of all data and how to split it up.
In FL, the data might be IID or non-IID.
Different learners can have varying amounts of data.
The number of learners in FL can be very dynamic.
Some devices might only join for a few training rounds or crash/fail/disconnect during training.
This comparison demonstrates that FL and DL have similarities but also crucial differences.

\subsubsection{FL Variety}
FL is a diverse discipline with various possible applications and use cases.
Most FL work focuses on end-user/edge/IoT devices.
FL is not exclusive to these environments and can work in conventional cloud environments.
As discussed in the first subsection, FL can train DNNs.
FL can also apply to classic ML models, such as linear models (logistic regression, classification, and more) or decision trees for explainable classifications.
FL also supports horizontal, vertical, and split learning.
This work omits discussing these techniques to avoid bloat.
More information about these and other methods is available in \cite{book:fl}.
Plenty of FL optimizations exist for each ML variant, such as custom algorithms and strategies.

Personalization can help if the global model is too general and does not satisfy a learner's individual needs.
Different personalized FL (PFL) approaches exist.
Some take the final trained global model and further train it on local data (fine-tuning).
Other techniques train two local models concurrently.
The first model gets shared and updated with the global parameters.
The second one stays isolated and only gets influenced by local data.
A mixture between the global and purely local model can be used for inference.
PFL is a deep and growing subfield of FL.
Helpful resources to learn more about PFL include \cite{book:fl,hpfl_over_massive_mobile_edge_computing_networks, paper:adaptive_exper_models_for_pfl}.


\subsubsection{FL Security \& Privacy}
The field of FL highly focuses on security and privacy because protecting those was the key motivation for FL's creation.
Secure FL should use secure and authenticated communication channels to prevent messages from being intercepted, read, or impersonated by a man-in-the-middle adversary.
To help with that, one should ensure that learners and aggregators are the only actors with access to those messages and can decipher them.

A variety of FL adversaries and threats exist.
Adversarial insiders, such as malicious aggregators or learners, are part of the FL process.
Adversarial outsiders try to interfere from beyond the FL system.
One threat example is manipulation, where insiders try to distort the model to their advantage.
Attackers tinker with FL components they can access.
The attack goals include polluting the global model to misclassify (Backdoor).
If the attack is untargeted (Byzantine), injecting random noise or flipping labels can degrade the model's performance.
It is difficult to detect malicious activity because FL can support dynamic or even unknown numbers of learners-
Each learner can use vastly different non-IID data.
It can be unclear if the learner is innocent and simply has access to unusual data or if the learner is adversarial.
Another example is if there are no safeguards in place during aggregation.
A malicious learner can claim to have used an overwhelming amount of training samples.
Thus, this learner's update overshadows other participants and influences the global model the most.
As a result, even very scarce, well-timed attacks in FL can have devastating impact. \cite{book:fl}

Another threat comes from (model) inference, where insiders or outsiders try to extract sensitive information about the used training data.
In classic FL, privacy leakage can only occur via inference.
Inference attacks try to deduce private information from artifacts that the FL process produces.
A large body of ML research exists that focuses on analyzing and protecting against such attacks.
There are different subtypes of inference attacks.
One example is the membership attack, which tries to find if specific samples were used for training.
The challenge here is that attackers have easy access to the final model.
Malicious insiders can even attack intermediate models.
Model inversion attacks are different attack variants in which adversaries query the trained model in peculiar ways to reverse engineer data samples. 
If the attacker is repeatedly successful, it is possible to deduce the original dataset.
Other attacks require malicious aggregators that can trace back the update parameters that the learner provided before aggregating the global parameters. \cite{book:fl}

Fortunately, there exists a growing array of defenses against those threats.
It is crucial to pick and combine these defenses wisely based on the use case and environment.
One major technique is differential privacy (DP).
DP is a complex mathematical framework that is formally proven to work.
One can use DP as noise for the dataset or (inference) query.
The downside is that DP might reduce the model accuracy significantly. \cite{book:fl}

Secure aggregation is a prominent protection against model inversion attacks.
It securely combines individual model parameters into global ones before sending them to the aggregator, which makes re-engineering and backtracking much harder \cite{paper:cluster-based-secure-aggregation-FL}. 