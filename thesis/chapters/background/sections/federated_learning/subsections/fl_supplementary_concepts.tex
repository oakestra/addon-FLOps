\subsection{Supplementary FL Concepts}

In this subsection we explore important supplementary FL concepts
to get a better understanding of the field.

\subsubsection{FL compared to Distributed Learning}

On first glance FL seems similar to Distributed Learning (DL).
Both are used for computational expensive large ML tasks.
To avoid the need for one extraordinary powerful machine,
and increase convergence times, 
the computations are distributed among many machines that
train individually. Afterward, a global model gets aggregated
at the server.

Now to the differences.
In FL, the quantity and distribution of training data
can be very diverse and might stay unknown for the entire FL process.
FL uses the data that the learners have. 
DL starts with full centralized access and control to the entirety of data,
before splitting it up among its fixed and predefined clients.
Thus, DL does not support the privacy concerns because it has
total oversight and control of all data and how it should be split up.
In FL the data might be IID or non-IID, different learners can have
different amounts of data.
The number of learners in FL can be very dynamic.
Some devices might only join for a few training rounds,
or crash/fail/disconnect during training.

\subsubsection{FL Variety}

The goal for this part is to showcase that FL can
be used for various different ML paradigms and use cases.

As we have already discussed in the first FL subsection,
FL can train DNNs.
FL is also applicable for classical ML models, such as
linear models (logistic regression, classification, and more) or
decision trees for explainable classifications.

FL can also support horizontal, vertical, and split learning.
Horizontal learning can be useful in scenarios where the available data features are the same but
originate from different sources.
For example, patient data from different hospitals
that records the same features like age, and ailment.
Vertical learning is useful when different data samples have
different feature spaces.
In the hospital example, this would mean asking different doctors/experts
about the same patients. 
The patient reports would be about the same individuals but include different features,
like cardiological metrics or neurological metrics.
We omit to discuss split learning due to its complexity that would bloat this thesis.

In case the global model is too general and does not satisfy the individual needs
of a learner, personalization can be employed.
Different personalized FL (PFL) approaches exist.
Some take the final trained global model and further train it on local data (fine tuning).
Others train two models locally.
One model that gets shared and updated.
The second one stays isolated and was only influenced by local data.
For inference a mixture between the global and purely local model can be used.
PFL is a deep and growing subfield of FL.

Plentiful FL optimizations, such as custom algorithms and strategies,
exist for each of these ML variants.

\subsubsection{FL Security \& Privacy}
Secure FL should use secure and authenticated
communication channels to avoid messages to be intercepted,
read, or impersonated via a man in the middle adversary.
To help with that one should ensure that 
learners and aggregators are the only actors that have
access to those messages and can decipher them.
There are two kinds of adversaries in FL.
Insiders that are part of the FL process, including
malicious aggregators or learners.
Or outsiders that try to interfere from beyond the FL system.

A variety of FL threats exist.
One example is manipulation where insiders try to distort
the model to their own advantage by tinkering with FL artifacts
it has access to.
The attacks goals include polluting the global model
to misclassify (Backdoor).
If the attack is un-targeted (Byzantine), by injecting random noise,
or flip labels, the model performance can degrade.

It is hard to detect malicious activity in because
FL can support dynamic or even unknown total number of learners
that can use vastly different non-IID data.
It can be unclear if the learner is innocent and simply has
access to unusual data, or if the learner is adversarial.
Another example is if there are no safeguards in place during aggregation,
a malicious learner can claim to have used an overwhelming amount of training samples,
thus overshadowing other participants and influencing the global model the most.
As a result, even very scarce, well timed attacks in FL can have devastating impact.

Another threat comes from inference, where 
insiders or outsiders try to extract sensitive information
about the used training data.
