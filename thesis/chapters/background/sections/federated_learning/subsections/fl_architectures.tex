% introduce more advanced concepts e.g. that "classic" FL is Cross-Device not Cross-Silo
% https://www.notion.so/oakestra-team/Chapter-9-Introduction-to-FL-Systems-f3c847d6899e4a58be9a1553635bc96c?pvs=4
% mostly only scratch the surface
% diff ways to handle larger FL systems
% Clustered FL
% Hierarchical FL <- talk a bit more indepth
% Decentralized FL
% Asynch FL
% https://www.notion.so/oakestra-team/Chapter-10-Local-Training-Scalability-of-FL-Systems-c0f945e2593e48ea9bdbb71c1d94d091?pvs=4
\subsection{FL Architectures}

FL comes in two broad structural categories.
Cross-silo or enterprise FL gets used for example in large data-centers or multinational companies.
Each learner represent a single large institution or participating group.
There are only around ten to a few dozen learners involved.
The identity of the parties are considered for training and verification.
In general, every individual local update from every learner at every training round is significant.
Fallouts and failures of individual learners are serious.

Cross-device FL can include hundreds or millions of devices, primarily edge/IoT devices.
One can say that cross-device is the opposite of cross-silo.
Due to this large pool of learners, typically one a subset of them get used per training round.
The identities of the participating learners are usually unimportant and get ignored.
Due to the nature of these devices and their environments, cross-device FL
needs to manage challenges, such as non-IID data, heterogenous device-hardware,
different network conditions, learner outages, or stragglers.
Various techniques exist to navigate these challenging conditions,
including specialized algorithms for aggregation or learner selection.
These strategies can take bias, availability, resources, and battery-life into account.
FLOps focuses on cross-device FL.
From now on, when we mention FL, we mean cross-device FL.

As discussed FLOps wants to benefit from the unique three-tiered Oakestra \cite{paper:oakestra_usenix} architecture.
Different FL architectures exist to support such large-scale FL environments.
The two main challenges for such scenarios are how o manage huge number of connections and aggregations,
and how to reduce the negative impact of straggling learner updates.
The problem with using a single aggregator, as seen in \ref{fig:basic_fl_intro}, is
that this single aggregator becomes a communication bottleneck.
Additionally, per-round training latency is limited by the slowest participating learner,
thus stragglers turn into another bottleneck.

We discuss four main architectures for large-scale FL.

\subsubsection{Clustered FL}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{clustered_fl.png}
    \caption{Clustered FL Architecture}
    \label{fig:clustered_fl}
\end{figure}
Figure \ref{fig:clustered_fl} shows the Clustered FL (CFL) architecture,
where similar learners are grouped together.
This clustering can be based on local data distribution, training latency,
available hardware, or geographical location.
The issue of the singular aggregator as a bottleneck persists.

The main challenge for CFL is choosing a fitting clustering criteria and strategy for the concrete use case.
If the criteria is very biased the risk arises to heavily favor updates from
preferred clusters, thus resulting in a biased global model with bad generalization.
Another task is to properly profile the nodes so that they are matched to the correct cluster.
For example if a slow outlier is present in a cluster the entire cluster suffers.
Node properties can very over time, so cluster membership has to be dynamic.
One should not overdue profiling otherwise privacy might get in danger.

The benefits of CFL are its ease of implementation,
familiar architecture to classic FL,
flexibility to tune clustering/selection dynamically,
and that CFL can be combined with other architectures.

The downsides of CFL are that a proper clustering strategy is 
use-case dependent and challenging to optimize.
CFL does not really solve scalability issues on its own,
especially that with larger numbers of nodes the clustering overhead becomes
critical.

\subsubsection{Hierarchical FL}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{hfl_architecture.png}
    \caption{Hierarchical FL Architecture}
    \label{fig:hfl_architecture}
\end{figure}
Figure \ref{fig:hfl_architecture} depicts the hierarchical FL (HFL) architecture.
In HFL the root aggregator delegates and distributes the aggregation task to 
intermediate aggregators.
Note that HFL can have multiple layers of intermediate aggregators.
Each intermediate aggregator and its connected learners resemble an instance of classic FL.
After aggregating an intermediate model the intermediate aggregators send their parameters
upstream to the root aggregator.
The root combines the intermediate parameters into global ones and sends them downstream for further FL rounds.
This structure requires significant modifications to the underlying FL architecture.
The proper design and implementation as well as assignment of learners to aggregators
determine the success of one's FL setup.
For example if too many learners are attached to a given aggregator, that aggregator becomes a bottleneck.
If too few learners are assigned that the intermediate aggregated model can get
very biased and the infrastructure resource and management costs become unjustified for the little number of learners.
With more components a management overhead arises, including handling fault-tolerance,
monitoring, synchronizing, and balancing.
Bad synchronization can amplify straggler problems.
Balancing refers to combining and harmonizing intermediate parameters to
get a good global model.

The benefits of HFL are its dynamic scalability and load balancing.
One can easily add or remove intermediate aggregators and their connected learners.
Due to this distribution of load and aggregation each individual aggregator including the root,
is less likely to face bottleneck issues.

HFL can be combined with CFL, where each intermediate aggregator is responsible
for one or multiple clusters.

The downsides of HFL are communication and management overheads.
More components lead to more messages being transmitted.
These messages all need to be secured and encrypted.
With more components and nodes more possible backdoors exist for adversaries to take advantage of.

\subsubsection{Decentralized FL}
Decentralized FL does not require a central aggregator, instead
it operates on a peer-to-peer basis via a blockchain.
That way the centralized communication bottleneck gets resolved.
The blockchain represents the global model.
Learners train in parallel.
Each locally trained update gets a version.
Based on this version random clients are chosen for aggregation.
The results gets appended to the blockchain, and the model version incremented.
FLOps does not use this kind of FL so we keep this part short.

\subsubsection{Asynchronous FL}
This architecture allows learners to train all the time and push
their updates to the aggregator once they are finished.
This method eliminates stragglers and dropout problems, because
a training round does not need to wait or handle for any outliers and timeouts.
The new issue of staleness arises, where updates get merged into the global model
that took a very long time to complete.
Such an update used a now outdated version of the global model.
As a result the global model gets partially reverted to an older state.
Asynchronous FL can be combined with other architectures.